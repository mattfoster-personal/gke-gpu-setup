apiVersion: batch/v1
kind: Job
metadata:
  name: faulty-job
  namespace: default
spec:
  completions: 3
  parallelism: 3
  template:
    spec:
      restartPolicy: Never
      containers:
        - name: gpu-test-container
          image: tensorflow/tensorflow:2.15.0-gpu
          resources:
            limits:
              nvidia.com/gpu: 1
          command: ["python3", "-c"]
          args:
            - |
              import torch
              print(f"Running on device: {torch.cuda.get_device_name(0)}")
      nodeSelector:
        kubernetes.io/hostname: <t4-gpu-node-name>  # Forces all jobs to one functional node
        cloud.google.com/gke-accelerator: "nvidia-tesla-t4"
