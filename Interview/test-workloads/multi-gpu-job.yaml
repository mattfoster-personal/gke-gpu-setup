apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: multi-gpu-mpi
spec:
  slotsPerWorker: 1  # Each worker will use 1 GPU
  runPolicy:
    cleanPodPolicy: Running
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          containers:
            - name: mpi-launcher
              image: tensorflow/tensorflow:latest-gpu
              command: ["/usr/bin/mpirun"]
              args:
                - "-np"
                - "2"  # Use 2 workers
                - "--hostfile"
                - "/etc/mpi/hostfile"
                - "python3"
                - "-c"
                - |
                  import tensorflow as tf
                  import time

                  gpus = tf.config.list_physical_devices('GPU')
                  if gpus:
                      try:
                          for gpu in gpus:
                              tf.config.experimental.set_memory_growth(gpu, True)
                          tf.config.set_visible_devices(gpus, 'GPU')
                          print("Using GPUs:", gpus)
                      except RuntimeError as e:
                          print(e)

                  strategy = tf.distribute.MirroredStrategy()
                  print(f"GPUs available: {strategy.num_replicas_in_sync}")

                  # Heavy computation - Large matrix multiplication loop
                  A = tf.random.normal([10000, 10000])
                  B = tf.random.normal([10000, 10000])

                  for i in range(500):  # Increase iterations to extend runtime
                      C = tf.matmul(A, B)
                      print(f"Iteration {i}: Computation completed")

                      if i % 50 == 0:
                          print(f"Checkpoint at iteration {i}")

                      time.sleep(1)  # Simulate workload over time

                  print("Finished computation")
    Worker:
      replicas: 2  # 2 worker pods
      template:
        spec:
          containers:
            - name: mpi-worker
              image: uber/horovod:latest
              resources:
                limits:
                  nvidia.com/gpu: 1  # Each worker gets 1 GPU
